{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "engstopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'very', \"shan't\", 'am', 'i', 'up', \"isn't\", 'himself', 'they', 'this', 'why', 'own', 'hadn', 'should', \"mustn't\", 'above', 'such', \"that'll\", 'ain', 'can', \"hadn't\", 'your', \"you've\", 'because', \"you're\", 'had', 'with', 'where', 'no', \"aren't\", 'while', 'too', 'yourselves', 'their', 'ourselves', 'shan', 'a', 'during', 'yours', 'does', 'over', 'below', 'doesn', 'what', 'then', 'before', 'themselves', 'herself', 'so', 'when', \"you'd\", 'who', 'o', 'by', \"should've\", 'to', 'as', 'down', 'on', \"she's\", \"wouldn't\", 'in', 'hasn', 'nor', 'his', 'just', 'how', 'm', 'are', 'we', 'wasn', 'my', 'its', 'doing', 'is', 'each', 'and', 'again', 't', 'off', 'other', 'our', 'shouldn', 'was', 'those', 'here', \"it's\", 'both', \"wasn't\", 'any', 'been', 'weren', 'not', 'mustn', 'him', 'most', 'few', 'into', 's', 'he', 'be', \"doesn't\", 'now', 'me', \"didn't\", 'at', 'have', 'some', 'couldn', \"mightn't\", 'yourself', 'about', 'has', 'after', 'same', 'you', 'did', \"weren't\", 'ma', 'all', 'don', 'ours', 'that', 'wouldn', 'than', \"needn't\", 'mightn', 'once', 'haven', 'these', \"hasn't\", 'isn', 'it', \"shouldn't\", 'there', 'the', 'through', 'if', 'which', 'further', 'until', 'more', 'hers', 'aren', 'only', 'for', 're', 'do', 'myself', 'of', 'her', 'she', 'theirs', \"you'll\", 'y', 'needn', 'won', 'itself', \"haven't\", 'under', 'an', \"don't\", 'but', 'd', 'them', 'having', 'were', 'will', 'whom', \"won't\", 'from', 'or', 'didn', 'being', 'between', 'll', 'against', \"couldn't\", 'out', 've'}\n"
     ]
    }
   ],
   "source": [
    "print(engstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fucntion to conver pos_tag part of speech tags to WordNet tags for use with lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes raw text, tokenizes it. Removes stopwords and words that are one character long( like 'a' and punctuation.) Returns a list of tokens.\n",
    "def tokenize(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1] \n",
    "    tokens = [word for word in tokens if word not in engstopwords] # remove stop words\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes token list and gets the part of speech for each. Returns a list of tuples ('word', 'POS')\n",
    "def get_pos(tokens):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizes words. That is, it normalizes words to their most\n",
    "basic form. For example, 'is', 'am' and 'are' are merged into 'be'.\n",
    "Returns a list of lemmatized tokens.\n",
    "\n",
    "Lemmatization is confined to parts of speech. Verb variations are lemmatized\n",
    "to the root verb, and same with nouns.\n",
    "\n",
    "Example: 'continues' and 'continuing' are merged to 'continue'\n",
    "but 'continuation' stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tagged_words):\n",
    "    lemma_list = []\n",
    "    for word, tag in tagged_words:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            lemma = lemmatizer.lemmatize(word) \n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the most common words in a token list. Returns a list of tuples ('word', frequency)\n",
    "def get_distfreq(tokens, top_n): \n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    return fdist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('A:\\\\Data Analysis Jupyter\\\\ML-Code\\\\NLP Concepts\\\\Tokenizing\\\\basictext.txt', encoding='utf-8') as f1:\n",
    "    coderre = f1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffcontinuing',\n",
       " 'progress',\n",
       " 'together',\n",
       " 'strong',\n",
       " 'plan',\n",
       " 'sustainable',\n",
       " 'city',\n",
       " 'electoral',\n",
       " 'platform',\n",
       " '2017-2021',\n",
       " 'dear',\n",
       " 'friends',\n",
       " 'last',\n",
       " 'four',\n",
       " 'years',\n",
       " 'proven',\n",
       " 'anything',\n",
       " 'working',\n",
       " 'together',\n",
       " 'nothing',\n",
       " 'impossible',\n",
       " 'since',\n",
       " 'arrival',\n",
       " 'city',\n",
       " 'hall',\n",
       " 'many',\n",
       " 'achievements',\n",
       " 'result',\n",
       " 'strong',\n",
       " 'commitment',\n",
       " 'elected',\n",
       " 'officials',\n",
       " 'teamed',\n",
       " 'community',\n",
       " 'leaders',\n",
       " 'montréal',\n",
       " 'civil',\n",
       " 'society',\n",
       " 'everyone',\n",
       " 'agreed']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize Coderre text and print first 40 tokens\n",
    "\n",
    "coderre_tokens = tokenize(coderre)\n",
    "coderre_tokens[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\ufeffcontinuing', 'VBG'),\n",
       " ('progress', 'NN'),\n",
       " ('together', 'RB'),\n",
       " ('strong', 'JJ'),\n",
       " ('plan', 'NN'),\n",
       " ('sustainable', 'JJ'),\n",
       " ('city', 'NN'),\n",
       " ('electoral', 'JJ'),\n",
       " ('platform', 'NN'),\n",
       " ('2017-2021', 'JJ'),\n",
       " ('dear', 'JJ'),\n",
       " ('friends', 'NNS'),\n",
       " ('last', 'JJ'),\n",
       " ('four', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('proven', 'RB'),\n",
       " ('anything', 'NN'),\n",
       " ('working', 'VBG'),\n",
       " ('together', 'RB'),\n",
       " ('nothing', 'NN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the part of speech for each token\n",
    "\n",
    "coderre_pos = get_pos(coderre_tokens)\n",
    "coderre_pos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('montréal', 99),\n",
       " ('city', 82),\n",
       " ('development', 36),\n",
       " ('new', 34),\n",
       " ('continue', 31),\n",
       " ('project', 28),\n",
       " ('develop', 28),\n",
       " ('work', 27),\n",
       " ('social', 26),\n",
       " ('public', 26)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the tokens using the wordnet POS converter function and get 10 most common words\n",
    "\n",
    "coderre_lemma = lemmatize(coderre_pos)\n",
    "get_distfreq(coderre_lemma, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
